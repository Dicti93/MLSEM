{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOIwwDzYzpUtxlEI1FQgQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dicti93/MLSEM/blob/master/CNN_ohne_Data_Leakage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1EoomVgyeO3"
      },
      "outputs": [],
      "source": [
        "# ### Imports and Setup\n",
        "# Importing necessary libraries and setting up configurations\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# TensorBoard writer\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.width', 230)\n",
        "\n",
        "# Seed settings\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.manual_seed_all(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Constants\n",
        "RAW_IMAGE_PATH = '/content/US_Pictures/US_Pictures/'\n",
        "IMAGE_PATH = '/content/US_Pictures/US_Pictures/Preprocessed/'\n",
        "ZIP_PATH = '/content/US_Pictures.zip'\n",
        "DATA_URL = \"https://raw.githubusercontent.com/Dicti93/MLSEM/master/2.%20Data%20and%20Description/data.csv\"\n",
        "MODEL_PATH = 'model.pth'\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "THRESHOLD = 10\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "# Define transformations\n",
        "TRANSFORM = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.7105, 0.7467, 0.3402], std=[0.2721, 0.3070, 0.2304])\n",
        "])\n",
        "\n",
        "# ### Directory Setup and Data Download\n",
        "# This section ensures that the required directories exist and downloads the data if necessary.\n",
        "\n",
        "# Ensure the required directories exist\n",
        "def ensure_directory_exists(path):\n",
        "    \"\"\"Create directory if it does not exist.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        logger.info(f\"Directory created: {path}\")\n",
        "        print(f\"Directory created: {path}\")\n",
        "\n",
        "# Download and extract data if necessary\n",
        "def download_and_extract_data():\n",
        "    \"\"\"Download and extract data if not already present.\"\"\"\n",
        "    if not os.listdir(RAW_IMAGE_PATH) or not any(f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')) for f in os.listdir(RAW_IMAGE_PATH)):\n",
        "        logger.info(\"Downloading data...\")\n",
        "        print(\"Downloading data...\")\n",
        "        os.system(f'wget -O {ZIP_PATH} https://zenodo.org/records/7669442/files/US_Pictures.zip')\n",
        "        os.system(f'unzip -q {ZIP_PATH} -d /content/US_Pictures/')\n",
        "    else:\n",
        "        logger.info(\"Data already downloaded.\")\n",
        "        print(\"Data already downloaded.\")\n",
        "\n",
        "# ### Data Display\n",
        "# This function helps visualize a few images from the dataset.\n",
        "\n",
        "def display_images(image_path, num_images=10):\n",
        "    \"\"\"Display a specified number of images from a directory.\"\"\"\n",
        "    files = [f for f in os.listdir(image_path) if f.endswith(('.png', '.bmp'))][:num_images]\n",
        "    if not files:\n",
        "        logger.warning(\"No image files found in the directory.\")\n",
        "        print(\"No image files found in the directory.\")\n",
        "        return\n",
        "    fig, axs = plt.subplots(1, num_images, figsize=(15, 5))\n",
        "    for i, file in enumerate(files):\n",
        "        img = Image.open(os.path.join(image_path, file))\n",
        "        axs[i].imshow(img)\n",
        "        axs[i].axis('off')\n",
        "        axs[i].set_title(file)\n",
        "    plt.show()\n",
        "\n",
        "# ### Data Loading and Preprocessing\n",
        "# This function loads and preprocesses the dataset.\n",
        "\n",
        "def load_and_preprocess_data(data_url):\n",
        "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    data = pd.read_csv(data_url, delimiter=';')\n",
        "    columns_to_drop = [\n",
        "        'Age', 'BMI', 'Sex', 'Height', 'Weight', 'Length_of_Stay', 'Management', 'Severity',\n",
        "        'Diagnosis_Presumptive', 'Alvarado_Score', 'Paedriatic_Appendicitis_Score', 'Appendix_on_US',\n",
        "        'Appendix_Diameter', 'Migratory_Pain', 'Lower_Right_Abd_Pain', 'Contralateral_Rebound_Tenderness',\n",
        "        'Coughing_Pain', 'Nausea', 'Loss_of_Appetite', 'Body_Temperature', 'WBC_Count', 'Neutrophil_Percentage',\n",
        "        'Segmented_Neutrophils', 'Neutrophilia', 'RBC_Count', 'Hemoglobin', 'RDW', 'Thrombocyte_Count',\n",
        "        'Ketones_in_Urine', 'RBC_in_Urine', 'WBC_in_Urine', 'CRP', 'Dysuria', 'Stool', 'Peritonitis',\n",
        "        'Psoas_Sign', 'Ipsilateral_Rebound_Tenderness', 'US_Performed', 'Free_Fluids', 'Appendix_Wall_Layers',\n",
        "        'Target_Sign', 'Appendicolith', 'Perfusion', 'Perforation', 'Surrounding_Tissue_Reaction', 'Appendicular_Abscess',\n",
        "        'Abscess_Location', 'Pathological_Lymph_Nodes', 'Lymph_Nodes_Location', 'Bowel_Wall_Thickening',\n",
        "        'Conglomerate_of_Bowel_Loops', 'Ileus', 'Coprostasis', 'Meteorism', 'Enteritis', 'Gynecological_Findings'\n",
        "    ] + [f'Unnamed: {i}' for i in range(58, 69)]\n",
        "    data.drop(columns=columns_to_drop, inplace=True)\n",
        "    data['Diagnosis'].replace({'appendicitis': 1, 'no appendicitis': 0, np.nan: 0}, inplace=True)\n",
        "    data.dropna(subset=['US_Number'], inplace=True)\n",
        "    data['US_Number'] = data['US_Number'].astype(int)\n",
        "    data['Diagnosis'] = data['Diagnosis'].astype(int)\n",
        "    id_to_label = {int(row['US_Number']): row['Diagnosis'] for index, row in data.iterrows()}\n",
        "    print(f\"Loaded and preprocessed data with {len(id_to_label)} entries\")\n",
        "    return id_to_label\n",
        "\n",
        "# ### Medical Images Dataset\n",
        "# This class handles the loading and transformation of medical images.\n",
        "\n",
        "class MedicalImagesDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_dict, transform=None):\n",
        "        \"\"\"Initialize the dataset with images and labels.\"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.labels_dict = labels_dict\n",
        "        self.transform = transform\n",
        "        self.filename_mapping = {}\n",
        "        self.unique_us_numbers = set()\n",
        "        self.diagnosis_count_per_image = defaultdict(int)\n",
        "        self.num_images_found = 0\n",
        "\n",
        "        print(\"Initializing MedicalImagesDataset...\")\n",
        "        for filename in os.listdir(root_dir):\n",
        "            self.num_images_found += 1\n",
        "            base_name = filename.split(' ')[0]\n",
        "            match = re.match(r\"([0-9]+(?:\\.[0-9]+)?)\", base_name)\n",
        "            if match:\n",
        "                image_number = int(match.group(1).split('.')[0])\n",
        "                label = labels_dict.get(image_number)\n",
        "                if label is not None:\n",
        "                    self.unique_us_numbers.add(image_number)\n",
        "                    self.diagnosis_count_per_image[label] += 1\n",
        "                    image_path = os.path.join(root_dir, filename)\n",
        "                    image = Image.open(image_path).convert('RGB')\n",
        "                    processed_image = self.transform(image) if self.transform else image\n",
        "                    self.filename_mapping[filename] = (processed_image, label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the length of the dataset.\"\"\"\n",
        "        return len(self.filename_mapping)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get an item from the dataset by index.\"\"\"\n",
        "        filename = list(self.filename_mapping.keys())[idx]\n",
        "        image, label = self.filename_mapping[filename]\n",
        "        return image, label, filename\n",
        "\n",
        "    def print_dataset_statistics(self):\n",
        "        \"\"\"Print statistics about the dataset.\"\"\"\n",
        "        logger.info(f\"Total images found before matching: {self.num_images_found}\")\n",
        "        logger.info(f\"Total US numbers successfully mapped to images: {len(self.unique_us_numbers)}\")\n",
        "        logger.info(f\"Total images successfully mapped to a US_NUMBER: {len(self.filename_mapping)}\")\n",
        "        for label, count in self.diagnosis_count_per_image.items():\n",
        "            logger.info(f\"Label {label}: {count} images\")\n",
        "\n",
        "# ### Data Augmentation and Dataset Splitting\n",
        "# This section handles the augmentation of images and splitting the dataset into training and testing sets.\n",
        "\n",
        "def augment_image(image, transform, times):\n",
        "    \"\"\"Apply augmentations to an image a specified number of times.\"\"\"\n",
        "    possible_transformations = [\n",
        "        lambda x: TF.rotate(x, -10),\n",
        "        lambda x: TF.rotate(x, -5),\n",
        "        lambda x: TF.rotate(x, 5),\n",
        "        lambda x: TF.rotate(x, 10),\n",
        "        lambda x: TF.affine(x, translate=(0.1, 0.1), scale=1.05, angle=0, shear=0),\n",
        "        lambda x: TF.affine(x, translate=(0.1, 0.1), scale=1.1, angle=0, shear=0),\n",
        "        lambda x: TF.affine(x, translate=(-0.1, -0.1), scale=0.95, angle=0, shear=0),\n",
        "        lambda x: TF.affine(x, translate=(-0.1, -0.1), scale=0.9, angle=0, shear=0),\n",
        "        lambda x: TF.affine(x, angle=0, translate=(0, 0), scale=1.0, shear=5)\n",
        "    ]\n",
        "    selected_transforms = random.sample(possible_transformations, min(len(possible_transformations), times))\n",
        "    augmented_images = [transform(transform(image)) for transform in selected_transforms]\n",
        "    return augmented_images\n",
        "\n",
        "def create_train_test_datasets(dataset, test_size=0.2, augment_times=2):\n",
        "    \"\"\"Create training and testing datasets with augmentation.\"\"\"\n",
        "    images, labels = [], []\n",
        "    for img, label, filename in dataset:\n",
        "        images.append((img, label, filename))\n",
        "        labels.append(label)\n",
        "\n",
        "    train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "        images, labels, test_size=test_size, stratify=labels, random_state=1234\n",
        "    )\n",
        "\n",
        "    augmented_images = []\n",
        "    for img, label, filename in train_images:\n",
        "        if label == 0:\n",
        "            aug_imgs = augment_image(TF.to_pil_image(img), TRANSFORM, augment_times)\n",
        "            augmented_images.extend([(aug_img, label, f\"{filename}_aug{i}\") for i, aug_img in enumerate(aug_imgs)])\n",
        "\n",
        "    train_images.extend(augmented_images)\n",
        "\n",
        "    train_dataset = DatasetSubset(train_images)\n",
        "    test_dataset = DatasetSubset(test_images)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "class DatasetSubset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        \"\"\"Initialize the dataset subset.\"\"\"\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the length of the dataset subset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get an item from the dataset subset by index.\"\"\"\n",
        "        return self.data[idx]\n",
        "\n",
        "def pad_sequence(sequences):\n",
        "    \"\"\"Pad sequences to the same length.\"\"\"\n",
        "    max_len = max([seq.size(0) for seq in sequences])\n",
        "    batch_size, c, h, w = sequences[0].size(0), sequences[0].size(1), sequences[0].size(2), sequences[0].size(3)\n",
        "    padded_sequences = torch.zeros(len(sequences), max_len, c, h, w)\n",
        "    for i, seq in enumerate(sequences):\n",
        "        padded_sequences[i, :seq.size(0)] = seq\n",
        "    return padded_sequences\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Custom collate function for DataLoader.\"\"\"\n",
        "    images, labels, patient_ids = zip(*batch)\n",
        "    padded_images = pad_sequence(images)\n",
        "    labels = torch.tensor(labels)\n",
        "    patient_ids = torch.tensor(patient_ids)\n",
        "    return padded_images, labels, patient_ids\n",
        "\n",
        "# ### Patient Dataset and DataLoader Creation\n",
        "# This section handles the creation of patient datasets and DataLoaders.\n",
        "\n",
        "class PatientDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        \"\"\"Initialize the patient dataset.\"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.patient_data = defaultdict(list)\n",
        "\n",
        "        print(\"Initializing PatientDataset...\")\n",
        "        for img, label, filename in dataset:\n",
        "            patient_id = int(filename.split('.')[0])\n",
        "            self.patient_data[patient_id].append((img, label, filename))\n",
        "\n",
        "        self.patient_ids = list(self.patient_data.keys())\n",
        "        print(f\"Initialized PatientDataset with {len(self.patient_ids)} patients.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the length of the patient dataset.\"\"\"\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get an item from the patient dataset by index.\"\"\"\n",
        "        patient_id = self.patient_ids[idx]\n",
        "        patient_images = [x[0] for x in self.patient_data[patient_id]]\n",
        "        patient_images = [img if isinstance(img, torch.Tensor) else TRANSFORM(img) for img in patient_images]\n",
        "        patient_label = self.patient_data[patient_id][0][1]\n",
        "        patient_images = torch.stack(patient_images)\n",
        "        return patient_images, patient_label, patient_id\n",
        "\n",
        "def create_dataloaders(dataset, batch_size, n_splits=5):\n",
        "    \"\"\"Create DataLoaders for training, validation, and testing.\"\"\"\n",
        "    skf = StratifiedKFold(n_splits=n_splits)\n",
        "    patient_ids = np.array(dataset.patient_ids)\n",
        "    labels = np.array([dataset.patient_data[pid][0][1] for pid in patient_ids])\n",
        "\n",
        "    for train_index, test_index in skf.split(patient_ids, labels):\n",
        "        train_ids, test_ids = patient_ids[train_index], patient_ids[test_index]\n",
        "\n",
        "        train_dataset = Subset(dataset, [dataset.patient_ids.index(pid) for pid in train_ids])\n",
        "        test_dataset = Subset(dataset, [dataset.patient_ids.index(pid) for pid in test_ids])\n",
        "\n",
        "        val_size = int(0.2 * len(train_dataset))\n",
        "        train_size = len(train_dataset) - val_size\n",
        "        train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "        return train_loader, val_loader, test_loader, test_dataset\n",
        "\n",
        "# ### Model Definition and Training\n",
        "# This section defines the CNN model and functions for training and evaluating the model.\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the CNN model.\"\"\"\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 64 * 64, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the model.\"\"\"\n",
        "        batch_size, num_images, c, h, w = x.size()\n",
        "        x = x.view(batch_size * num_images, c, h, w)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(batch_size, num_images, -1)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "def plot_roc_and_recall(preval_labels, preval_preds, postval_labels=None, postval_preds=None):\n",
        "    \"\"\"Plot ROC and Precision-Recall curves.\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(preval_labels, preval_preds)\n",
        "    roc_auc = roc_auc_score(preval_labels, preval_preds)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (pre-training area = {roc_auc:.2f})')\n",
        "    if postval_labels is not None and postval_preds is not None:\n",
        "        fpr, tpr, _ = roc_curve(postval_labels, postval_preds)\n",
        "        post_roc_auc = roc_auc_score(postval_labels, postval_preds)\n",
        "        plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (post-training area = {post_roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(preval_labels, preval_preds)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(recall, precision, color='darkorange', lw=2, label='Pre-training curve')\n",
        "    if postval_labels is not None and postval_preds is not None:\n",
        "        precision, recall, _ = precision_recall_curve(postval_labels, postval_preds)\n",
        "        plt.plot(recall, precision, color='blue', lw=2, label='Post-training curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall curve')\n",
        "    plt.legend(loc='lower left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, model_path, patience):\n",
        "    \"\"\"Train the model with early stopping.\"\"\"\n",
        "    best_val_roc_auc = 0\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    pretrain_loss = 0.0\n",
        "    pretrain_preds, pretrain_labels = [], []\n",
        "    for images, labels, _ in tqdm(train_loader):\n",
        "        outputs = model(images)\n",
        "        labels = labels.float().view(-1, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        pretrain_loss += loss.item()\n",
        "        pretrain_preds.extend(outputs.detach().cpu().numpy())\n",
        "        pretrain_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "    pretrain_loss /= len(train_loader)\n",
        "    pretrain_acc = accuracy_score(pretrain_labels, (np.array(pretrain_preds) > 0.5).astype(int))\n",
        "    pretrain_roc_auc = roc_auc_score(pretrain_labels, pretrain_preds)\n",
        "\n",
        "    preval_preds, preval_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _ in val_loader:\n",
        "            outputs = model(images)\n",
        "            preval_preds.extend(outputs.detach().cpu().numpy())\n",
        "            preval_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "    preval_acc = accuracy_score(preval_labels, (np.array(preval_preds) > 0.5).astype(int))\n",
        "    preval_roc_auc = roc_auc_score(preval_labels, preval_preds)\n",
        "\n",
        "    print(f\"Before training\")\n",
        "    print(f\"Train Loss: {pretrain_loss:.4f}, Train Acc: {pretrain_acc:.4f}, Train ROC AUC: {pretrain_roc_auc:.4f}\")\n",
        "    print(f\"Val Acc: {preval_acc:.4f}, Val ROC AUC: {preval_roc_auc:.4f}\")\n",
        "\n",
        "    plot_roc_and_recall(preval_labels, preval_preds)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds, train_labels = []\n",
        "\n",
        "        for images, labels, patient_ids in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\"):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            labels = labels.float().view(-1, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_preds.extend(outputs.detach().cpu().numpy())\n",
        "            train_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = accuracy_score(train_labels, (np.array(train_preds) > 0.5).astype(int))\n",
        "        train_roc_auc = roc_auc_score(train_labels, train_preds)\n",
        "\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "        writer.add_scalar('ROC_AUC/train', train_roc_auc, epoch)\n",
        "\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for images, labels, patient_ids in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\"):\n",
        "                outputs = model(images)\n",
        "                val_preds.extend(outputs.detach().cpu().numpy())\n",
        "                val_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        val_acc = accuracy_score(val_labels, (np.array(val_preds) > 0.5).astype(int))\n",
        "        val_roc_auc = roc_auc_score(val_labels, val_preds)\n",
        "\n",
        "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
        "        writer.add_scalar('ROC_AUC/val', val_roc_auc, epoch)\n",
        "\n",
        "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        logger.info(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train ROC AUC: {train_roc_auc:.4f}\")\n",
        "        logger.info(f\"Val Acc: {val_acc:.4f}, Val ROC AUC: {val_roc_auc:.4f}\")\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train ROC AUC: {train_roc_auc:.4f}\")\n",
        "        print(f\"Val Acc: {val_acc:.4f}, Val ROC AUC: {val_roc_auc:.4f}\")\n",
        "\n",
        "        if val_roc_auc > best_val_roc_auc:\n",
        "            best_val_roc_auc = val_roc_auc\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'train_roc_auc': train_roc_auc,\n",
        "                'val_acc': val_acc,\n",
        "                'val_roc_auc': val_roc_auc,\n",
        "            }, model_path)\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= patience:\n",
        "            logger.info(\"Early stopping triggered\")\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    plot_roc_and_recall(preval_labels, preval_preds, val_labels, val_preds)\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    \"\"\"Test the model on the test dataset.\"\"\"\n",
        "    model.eval()\n",
        "    test_preds, test_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels, patient_ids in tqdm(test_loader, desc=\"Testing\"):\n",
        "            outputs = model(images)\n",
        "            test_preds.extend(outputs.detach().cpu().numpy())\n",
        "            test_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "    test_acc = accuracy_score(test_labels, (np.array(test_preds) > 0.5).astype(int))\n",
        "    test_roc_auc = roc_auc_score(test_labels, test_preds)\n",
        "    logger.info(f\"Test Acc: {test_acc:.4f}, Test ROC AUC: {test_roc_auc:.4f}\")\n",
        "    print(f\"Test Acc: {test_acc:.4f}, Test ROC AUC: {test_roc_auc:.4f}\")\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"Load the model from the specified path.\"\"\"\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    train_loss = checkpoint['train_loss']\n",
        "    train_acc = checkpoint['train_acc']\n",
        "    train_roc_auc = checkpoint['train_roc_auc']\n",
        "    val_acc = checkpoint['val_acc']\n",
        "    val_roc_auc = checkpoint['val_roc_auc']\n",
        "    model.eval()\n",
        "    return epoch, train_loss, train_acc, train_roc_auc, val_acc, val_roc_auc\n",
        "\n",
        "def evaluate_model_on_testset_with_patient_ids(model, test_loader):\n",
        "    \"\"\"Evaluate the model on the test dataset with patient IDs.\"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, patient_ids in test_loader:\n",
        "            outputs = model(images)\n",
        "            predicted_labels = (outputs.detach().cpu().numpy() > 0.5).astype(int)\n",
        "            true_labels = labels.detach().cpu().numpy()\n",
        "            patient_ids = patient_ids.detach().cpu().numpy()\n",
        "\n",
        "            for i in range(len(true_labels)):\n",
        "                results.append((patient_ids[i], true_labels[i], predicted_labels[i][0]))\n",
        "\n",
        "    for patient_id, true_label, predicted_label in results:\n",
        "        logger.info(f\"Patient ID: {patient_id}, True Label: {true_label}, Predicted Label: {predicted_label}\")\n",
        "        print(f\"Patient ID: {patient_id}, True Label: {true_label}, Predicted Label: {predicted_label}\")\n",
        "\n",
        "# ## Main Execution\n",
        "# The main execution section that sets up directories, downloads data, and trains/tests the model.\n",
        "\n",
        "# Ensure directories exist and data is downloaded\n",
        "ensure_directory_exists(RAW_IMAGE_PATH)\n",
        "ensure_directory_exists(IMAGE_PATH)\n",
        "download_and_extract_data()\n",
        "\n",
        "# Display sample images\n",
        "display_images(RAW_IMAGE_PATH)\n",
        "\n",
        "# Load and preprocess data\n",
        "id_to_label = load_and_preprocess_data(DATA_URL)\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = MedicalImagesDataset(IMAGE_PATH, id_to_label, TRANSFORM)\n",
        "logger.info(f\"Dataset initialized with {len(dataset)} samples\")\n",
        "print(f\"Dataset initialized with {len(dataset)} samples\")\n",
        "\n",
        "# Check if dataset is empty\n",
        "if len(dataset) == 0:\n",
        "    raise ValueError(\"Dataset is empty. Please check the image loading process.\")\n",
        "\n",
        "# Create training and testing datasets\n",
        "train_dataset, test_dataset = create_train_test_datasets(dataset, test_size=0.2)\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Ensure test set contains both classes\n",
        "test_labels = [label for _, label, _ in test_dataset]\n",
        "assert len(set(test_labels)) == 2, \"Test set does not contain both classes!\"\n",
        "\n",
        "# Initialize patient dataset\n",
        "patient_dataset = PatientDataset(train_dataset)\n",
        "\n",
        "# Print dataset structure for inspection\n",
        "for idx in range(5):\n",
        "    patient_images, patient_label, patient_id = patient_dataset[idx]\n",
        "    logger.debug(f\"Patient ID: {patient_id}, Label: {patient_label}, Images Tensor Shape: {patient_images.shape}\")\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "train_loader, val_loader, test_loader, test_dataset = create_dataloaders(patient_dataset, BATCH_SIZE)\n",
        "\n",
        "# Print dataset sizes before training\n",
        "logger.info(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "logger.info(f\"Validation set size: {len(val_loader.dataset)}\")\n",
        "logger.info(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation set size: {len(val_loader.dataset)}\")\n",
        "print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS, MODEL_PATH, EARLY_STOPPING_PATIENCE)\n",
        "\n",
        "# Test the model\n",
        "test_model(model, test_loader)\n",
        "\n",
        "# Load the model\n",
        "epoch, train_loss, train_acc, train_roc_auc, val_acc, val_roc_auc = load_model(MODEL_PATH)\n",
        "\n",
        "# Evaluate model on test set with patient IDs\n",
        "test_loader_with_patient_ids = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
        "evaluate_model_on_testset_with_patient_ids(model, test_loader_with_patient_ids)\n",
        "\n",
        "# Close TensorBoard writer\n",
        "writer.close()\n"
      ]
    }
  ]
}